{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "NLTK_mini_homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOqWZg1s_t91"
      },
      "source": [
        "## Data Preprocessing Using NLTK\n",
        "The training data contains about 8,000 comments with corresponding stars(1 to 5). Assume that we're going to train a model to predict the star by the comment. In this homework, we're going to implement data preprocessing by using NLTK. The steps are shown below.\n",
        "<br>\n",
        "<li> Import the packages you need and read the csv file.\n",
        "<li> Turn each comment into a word bag. Remember the bag only contain verb and adjective, stop words and punctuations are excluded.\n",
        "<li> Turn the word bag into number using one-hot encoding. Each row represents the sample, and each column represent the word.\n",
        "<li> Finally, using the train_test_split function in sklearn.model_selection to split the data into training set and testing set. Then put the training set into the model.(This part of code is provided.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYD7nowLDAKN",
        "outputId": "5a9f5635-a6df-4178-c4c2-03cef8ad2cae"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo9SHmj0DCqG",
        "outputId": "fc856f2e-21de-4f27-cb6b-f1034000374f"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/bigdata/miniHW"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/bigdata/miniHW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "ryP0Ou-7_t-b",
        "outputId": "bbebe69d-af5f-4707-efcb-4308df5dcafe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"training_data.csv\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3223</td>\n",
              "      <td>2055</td>\n",
              "      <td>2533</td>\n",
              "      <td>Sometimes things happen, and when they do this...</td>\n",
              "      <td>2010/12/30</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9938</td>\n",
              "      <td>4165</td>\n",
              "      <td>6371</td>\n",
              "      <td>I know Kerrie through my networking and we ben...</td>\n",
              "      <td>2011/4/26</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7123</td>\n",
              "      <td>869</td>\n",
              "      <td>4929</td>\n",
              "      <td>Love their pizza!!!\\r\\nVery fresh. Their canno...</td>\n",
              "      <td>2012/9/28</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3601</td>\n",
              "      <td>1603</td>\n",
              "      <td>2789</td>\n",
              "      <td>Being from NJ I am always on the prowl for my ...</td>\n",
              "      <td>2009/6/7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3948</td>\n",
              "      <td>2347</td>\n",
              "      <td>1245</td>\n",
              "      <td>We have tried this spot a few times and each v...</td>\n",
              "      <td>2011/2/20</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_id  business_id  ...        date stars\n",
              "0       3223         2055  ...  2010/12/30     5\n",
              "1       9938         4165  ...   2011/4/26     5\n",
              "2       7123          869  ...   2012/9/28     5\n",
              "3       3601         1603  ...    2009/6/7     4\n",
              "4       3948         2347  ...   2011/2/20     4\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Npif_m_t-i"
      },
      "source": [
        "<li>Write a function to turn all the comments into wordbag, and pick up verbs and adjectives only.\n",
        "<br>1.input the \"text\" column in df (i.e. df.text), and tokenize all the comments(nltk.word_tokenize() )\n",
        "<br>2. pick up all the stop words and punctuation (string.punctuation and nltk.corpus.stopwords.words('english')  )\n",
        "<br>3. pos_tag the remain words, and pick up lemmatized verbs and  lemmatized adjectives only.(nltk.pos_tag()  and wnl.lemmatize())\n",
        "<br>4. return a list which contains dictionaries, each dictionary is a comment, i.e.\n",
        "<br>[{'happen': 1, 'want': 1, 'take': 1, 'best': 1, 'nice': 1, 'find': 1},\n",
        " {'know': 1,\n",
        "  'kerrie': 2,\n",
        "  'benefit': 1,\n",
        "  'need': 3,\n",
        "  'plan': 1,\n",
        "  'remind': 1,\n",
        "1},\n",
        " {'love': 1, 'fresh': 1, 'good': 1, 'seem': 1, 'great': 1},\n",
        " {'hometown': 1,\n",
        "  'italian': 1,\n",
        "  'best': 1,\n",
        "  'pizza': 1,\n",
        "  'big': 1,}]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o62eqDMJxqck",
        "outputId": "f845b574-faf7-4f97-8f3c-c9264e07cc7d"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9FVBg89uMAr"
      },
      "source": [
        "wnl = WordNetLemmatizer()\r\n",
        "def tokenize_document(list_text):\r\n",
        "    output = []\r\n",
        "    punct = string.punctuation\r\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "    tags_need = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\r\n",
        "    for sentence in list_text:\r\n",
        "      words = []\r\n",
        "      # 去除punctuation\r\n",
        "      for p in punct:\r\n",
        "         sentence = sentence.replace(p, '.') \r\n",
        "      # 去除stopwords, 找出動詞和形容詞\r\n",
        "      for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\r\n",
        "        if '.' in word:\r\n",
        "          word = word[:word.index('.')]\r\n",
        "        if (word not in stopwords) and (tag in tags_need):\r\n",
        "          word = wnl.lemmatize(word, 'v') if tag[0] == 'V' else wnl.lemmatize(word, 'a')\r\n",
        "          # print(word)\r\n",
        "          words.append(word)\r\n",
        "      dic = {}\r\n",
        "      for word in words:\r\n",
        "        dic[word] = 1 if word not in dic else dic[word] + 1\r\n",
        "      output.append(dic)\r\n",
        "    return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeDQf2aE_YkA",
        "outputId": "a7b564ca-2835-4c63-b9a3-c698ef7b5464"
      },
      "source": [
        "output = tokenize_document(df['text'][:3])\r\n",
        "output"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'best': 1, 'find': 1, 'happen': 1, 'nice': 1, 'take': 1, 'want': 1},\n",
              " {'benefit': 1,\n",
              "  'come': 1,\n",
              "  'good': 1,\n",
              "  'help': 1,\n",
              "  'know': 1,\n",
              "  'look': 1,\n",
              "  'mention': 1,\n",
              "  'need': 3,\n",
              "  'remind': 1,\n",
              "  'troubled': 1,\n",
              "  'true': 1},\n",
              " {'Love': 1, 'fresh': 1, 'good': 1, 'great': 1, 'seem': 1}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvufOO6_t-m"
      },
      "source": [
        "<li>Write a function to turn the bag of word into numeric numpy array by one-hot encoding method.\n",
        "<br>1. Input the list from the return of above function.\n",
        "<br>2. create a python set, called \"features\", containing all the word in all comments.  ex:{\"I\", \"have\", \"a\", \"dog\", \"cat\"}\n",
        "<br>3. create a nested list, called mat, containg the counts of word in each comment.  \n",
        "<br>ex:  [{\"I\":1, \"have\":1, \"a\":1, \"dog\":1}, {\"I\":1, \"have\":1, \"a\":1, \"cat\":1}] -->[[1,1,1,1,0], [1,1,1,0,1]]\n",
        "<br>4. put the nested into numpy.array() and return the array and \"features\" set as the function results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IpiacmbCCjY"
      },
      "source": [
        "def vectorize_mat(dics):\r\n",
        "    features = set()\r\n",
        "    for dic in dics:\r\n",
        "      for k in dic:\r\n",
        "        if k not in features:\r\n",
        "          features.add(k) \r\n",
        "    features = sorted(features)\r\n",
        "    mat = []\r\n",
        "    for dic in dics:\r\n",
        "      row = [0]*len(features)\r\n",
        "      for k in dic:\r\n",
        "        # print(k)\r\n",
        "        row[features.index(k)] = dic[k]\r\n",
        "      mat.append(row)\r\n",
        "    return features, np.array(mat)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLfe5_wlEUm_",
        "outputId": "e0eaca97-5aaf-4cec-de5f-407ce646c4fd"
      },
      "source": [
        "vectorize_mat(tokenize_document(df['text'][:3]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Love',\n",
              "  'benefit',\n",
              "  'best',\n",
              "  'come',\n",
              "  'find',\n",
              "  'fresh',\n",
              "  'good',\n",
              "  'great',\n",
              "  'happen',\n",
              "  'help',\n",
              "  'know',\n",
              "  'look',\n",
              "  'mention',\n",
              "  'need',\n",
              "  'nice',\n",
              "  'remind',\n",
              "  'seem',\n",
              "  'take',\n",
              "  'troubled',\n",
              "  'true',\n",
              "  'want'],\n",
              " array([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
              "        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 3, 0, 1, 0, 0, 1, 1, 0],\n",
              "        [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPrrVJbk_t-s"
      },
      "source": [
        "Here, we choose Multinomial Naive Bayes Classifier as the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8eLN31_t-t"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ERsQGVSyfZ"
      },
      "source": [
        "features, one_hot_mat = vectorize_mat(tokenize_document(df['text'][:100]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLWTneFwTlsC",
        "outputId": "e9cbd993-34e1-48f2-eb27-cb50ced16a8d"
      },
      "source": [
        "csr_matrix(one_hot_mat)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<100x749 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 1760 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv4AeROM_t-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1198f70-6d00-4cde-b423-5938b643fa4f"
      },
      "source": [
        "train_data, test_data, train_lab, test_lab = train_test_split(csr_matrix(one_hot_mat), df.stars[:100] , train_size = 0.8, random_state = 123)\n",
        "MNB_model = MultinomialNB(alpha = 0.5)\n",
        "MNB_model.fit(train_data, train_lab)\n",
        "MSE = np.std(MNB_model.predict(test_data) - test_lab)\n",
        "ACC = MNB_model.score(test_data, test_lab)\n",
        "print(\"Under MNB model, MSE and accuracy are %.3f and %.3f, respectively.\" % (MSE, ACC) )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Under MNB model, MSE and accuracy are 1.072 and 0.450, respectively.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}